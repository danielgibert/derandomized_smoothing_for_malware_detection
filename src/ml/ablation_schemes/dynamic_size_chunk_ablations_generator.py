import torch
from typing import Tuple
import random
import math
from random import randint
from src.ml.ablation_schemes.ablations_generator import AblatedEnd2EndGenerator


class DynamicSizeChunkAblatedEnd2EndGenerator(AblatedEnd2EndGenerator):
    def __init__(self, train_mode: bool = True, file_portion: float = 0.1, padding_value: float = 256.0, num_chunks: int = 100):
        super().__init__(train_mode)
        self.file_portion = file_portion
        self.padding_value = padding_value
        self.num_chunks = num_chunks  # To be used only during testing

    def generate_training_chunks(self, batch: list) -> Tuple[torch.Tensor, torch.Tensor]:
        max_size = max([x[0].size()[0] for x in batch])
        chunk_size = int(-(-(max_size * self.file_portion) // 1))  # Used to round up the float value

        vecs = []
        labels = []
        for x, y in batch:
            if x.shape[0] <= chunk_size:
                vecs.append(x)
            else:
                start_location = random.randint(0, max(0, x.shape[0] - chunk_size))
                end_location = start_location + chunk_size
                vecs.append(x[start_location:end_location])
            labels.append(y)
        x = torch.nn.utils.rnn.pad_sequence(vecs, batch_first=True, padding_value=self.padding_value)
        # stack will give us (B, 1), so index [:,0] to get to just (B)
        y = torch.stack(labels)[:, 0]

        return x, y

class SequentialDynamicSizeChunkAblatedEnd2EndGenerator(DynamicSizeChunkAblatedEnd2EndGenerator):
    def __init__(self, train_mode: bool = True, file_portion: float = 0.1, padding_value: float = 256.0,
                 num_chunks: int = 100):
        DynamicSizeChunkAblatedEnd2EndGenerator.__init__(
            self,
            train_mode,
            file_portion,
            padding_value,
            num_chunks
        )

    def generate_testing_chunks(self, batch: list) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        list_ = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']
        n = 3  # group size, in our case, m = len(l)*file_portion
        m = 1  # overlap size, in our case,  m = group_size - (int(len(l)/num_chunks))
        [list_[i:i+n] for i in range(0, len(list_), n-m)]

        :param batch:
        :return:
        """
        labels = []
        vecs = []
        if len(batch) == 1:  # Only implemented for batch sizes equals to 1
            max_size = batch[0][0].size()[0]

            group_size = math.ceil(max_size * self.file_portion)
            overlap_size = group_size - int(max_size / self.num_chunks)

            to_substract_A = math.ceil((group_size - overlap_size) / self.num_chunks)
            to_substract_B = math.ceil((group_size - overlap_size) * self.file_portion)
            to_substract = to_substract_B - to_substract_A

            x = batch[0][0]

            for i in range(self.num_chunks):
                start = i * (group_size - overlap_size - to_substract)
                end = start + group_size
                sublist = x[start:end]
                vecs.append(sublist)

            x = torch.nn.utils.rnn.pad_sequence(vecs, batch_first=True, padding_value=self.padding_value)
            labels.append(batch[0][1])
            y = torch.stack(labels)[:, 0]
            return x, y
        else:
            raise NotImplementedError

    def generate_ablated_versions(self, x: torch.Tensor) -> Tuple[torch.Tensor, list]:
        vecs = []
        locations = []
        print(x)
        max_size = x.size()[0]

        group_size = math.ceil(max_size * self.file_portion)
        overlap_size = group_size - int(max_size / self.num_chunks)

        a = math.ceil((group_size - overlap_size) / self.num_chunks)
        b = math.ceil((group_size - overlap_size) * self.file_portion)
        bytes_corrected = b - a # Need to correct this bytes from the starting location

        for i in range(self.num_chunks):
            start = i * (group_size - overlap_size - bytes_corrected)
            end = start + group_size
            sublist = x[start:end]
            locations.append([start, end])
            vecs.append(sublist)

        x = torch.nn.utils.rnn.pad_sequence(vecs, batch_first=True, padding_value=self.padding_value)
        return x, locations

class RandomDynamicSizeChunkAblatedEnd2EndGenerator(DynamicSizeChunkAblatedEnd2EndGenerator):
    def __init__(self, train_mode: bool = True, file_portion: float = 0.1, padding_value: float = 256.0,
                 num_chunks: int = 100):
        DynamicSizeChunkAblatedEnd2EndGenerator.__init__(
            self,
            train_mode,
            file_portion,
            padding_value,
            num_chunks
        )

    def generate_testing_chunks(self, batch: list) -> Tuple[torch.Tensor, torch.Tensor]:
        labels = []
        vecs = []
        if len(batch) == 1:  # Only implemented for batch sizes equals to 1
            max_size = batch[0][0].size()[0]
            chunk_size = int(-(-(max_size * self.file_portion) // 1))  # Used to round up the float value
            x = batch[0][0]
            indices_added = []

            for i in range(self.num_chunks):
                curr_idx = randint(0, len(x) - chunk_size)
                indices_added.append(curr_idx)
                vecs.append(x[curr_idx:curr_idx + chunk_size])
            x = torch.nn.utils.rnn.pad_sequence(vecs, batch_first=True, padding_value=self.padding_value)
            labels.append(batch[0][1])
            y = torch.stack(labels)[:, 0]
            return x, y
        else:
            raise NotImplementedError

    def generate_ablated_versions(self, x: torch.Tensor) -> Tuple[torch.Tensor, list]:
        vecs = []
        max_size = x.size()[0]
        chunk_size = int(-(-(max_size * self.file_portion) // 1))  # Used to round up the float value. e.g. 27.2->28

        locations = []
        for i in range(self.num_chunks):
            curr_idx = randint(0, len(x) - chunk_size)
            locations.append([curr_idx, curr_idx + chunk_size])
            vecs.append(x[curr_idx:curr_idx + chunk_size])
        x = torch.nn.utils.rnn.pad_sequence(vecs, batch_first=True, padding_value=self.padding_value)
        return x, locations